{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/judem-21/Neural-Style-Transfer/blob/main/NST2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qRPDq-O3Uqgq"
      },
      "outputs": [],
      "source": [
        "#importing all the necessary python libraries\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#importing all the optimisers such as Adam,momentum etc.\n",
        "import torch.optim as optim\n",
        "\n",
        "#importing the Python Image Library(PIL) to deal with loading of images and their file extension types\n",
        "from PIL import Image\n",
        "\n",
        "#to convert our images to vectors/pytorch tensors\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "#for importing the vgg 19 pretrained model\n",
        "import torchvision.models as models\n",
        "\n",
        "#to read,write and save images\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ribilPbVUx_d"
      },
      "outputs": [],
      "source": [
        "#selecting the layers for the content loss and style loss computation\n",
        "layers_content=[21,34]\n",
        "layers_style=[0,5,10,19,28]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlDEDq3jVNsR",
        "outputId": "6660a4e1-e6c9-4452-be0c-6fc59f334b7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#mounting google drive to access and store images\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBT0ZBd2WUM_"
      },
      "outputs": [],
      "source": [
        "#creating a python class whose instance would be our network i.e the VGG 19 model\n",
        "class VGG(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(VGG,self).__init__()\n",
        "    self.chosen_layers1=layers_content\n",
        "    self.chosen_layers2=layers_style\n",
        "    self.model=models.vgg19(pretrained=True).features\n",
        "\n",
        "  def forward(self,x,chk):\n",
        "    features_con=[]\n",
        "    features_sty=[]\n",
        "\n",
        "    #sending in the image through the model and storing activations of the respective layers of the content image style image and the generated image to compute the losses\n",
        "    for layer_num,layer in enumerate(self.model):\n",
        "\n",
        "      #passing the activations to the next layer\n",
        "      x=layer(x)\n",
        "\n",
        "      if layer_num in self.chosen_layers1:\n",
        "        features_con.append(x)\n",
        "\n",
        "      if layer_num in self.chosen_layers2:\n",
        "        features_sty.append(x)\n",
        "    if chk==0:\n",
        "      return features_con,features_sty\n",
        "    elif chk==1:\n",
        "      return features_con\n",
        "    else:\n",
        "      return features_sty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBFF2mLBW0bx"
      },
      "outputs": [],
      "source": [
        "#setting our primary computing device to gpu if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9o__B9IW2hQ"
      },
      "outputs": [],
      "source": [
        "#setting image size to 256 by 256\n",
        "image_size=256\n",
        "\n",
        "loader=transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_size,image_size)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMFvojuOW4SI"
      },
      "outputs": [],
      "source": [
        "#loading our image and transforming it to a pytorch tensor of the size that is acceptable by the model\n",
        "def load_image(image_name):\n",
        "  image = Image.open(image_name)\n",
        "\n",
        "  #adding an extra dimension of the batchsize which is 1\n",
        "  image = loader(image).unsqueeze(0)\n",
        "\n",
        "  #migrating the tensors to the respective device for computation\n",
        "  return image.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3y1pwiMW8uA"
      },
      "outputs": [],
      "source": [
        "#passing the image paths to the user defined function above in order to transform it to required tensors\n",
        "content_img=load_image('/content/drive/MyDrive/Conv_images/NST/catim.jpg')\n",
        "style_img=load_image('/content/drive/MyDrive/Conv_images/NST/conim.jpg')\n",
        "\n",
        "#generating a random image i.e. initialising a 256 by 256 by 3 tensor with random pixel values\n",
        "generated_img=torch.randn(content_img.shape,device=device,requires_grad=True)\n",
        "#generated_img=content_img.clone().requires_grad_(True)\n",
        "\n",
        "#setting the hyperparameters of the model\n",
        "total_steps=10000 #number of times the generated image is going to be modified i.e. total number of iterations\n",
        "learning_rate=0.7\n",
        "alpha=1000\n",
        "beta=1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56fG8XeEt-HE"
      },
      "outputs": [],
      "source": [
        "generated_img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMbaAphZXAvC"
      },
      "outputs": [],
      "source": [
        "#storing the instance/VGG class object and locking its weights and biases using eval function\n",
        "model=VGG().to(device).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nN6Ht4yMXBo9"
      },
      "outputs": [],
      "source": [
        "#setting the optimizer as Adam for better results\n",
        "optimizer=optim.Adam([generated_img],lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zhii0ymIXGC9"
      },
      "outputs": [],
      "source": [
        "stp=0\n",
        "\n",
        "#starting the training process upto the set number of iterations which is usually about 1000\n",
        "for step in range(total_steps):\n",
        "\n",
        "  #passing the randomly generated image, the content image\n",
        "  #and the style image through the instance of the VGG 19 class(user defined) to get the activations of the content and style layers\n",
        "  generated_con,generated_sty=model(generated_img,0)\n",
        "  content_features=model(content_img,1)\n",
        "  style_features=model(style_img,2)\n",
        "  #each of the above tensors have each element of theirs as a tensor of the activations of 1 layer\n",
        "  #in other words each element of the above tensors represenrts one entire layer(all channels included)\n",
        "\n",
        "  style_loss,content_loss,total_loss=0,0,0\n",
        "\n",
        "  #computing content loss with the activations of the content layers for the generated image and content image\n",
        "  for gen_c,contnt_feature in zip(generated_con,content_features):\n",
        "    batch_size,channel,height,width=gen_c.shape\n",
        "    content_loss+=torch.mean((gen_c-contnt_feature)**2)*0.5\n",
        "\n",
        "    #computing style loss with the activations of the style layers for the generated image and style image\n",
        "  for gen_s,style_feature in zip(generated_sty,style_features):\n",
        "    batch_size,channel,height,width=gen_s.shape\n",
        "    G=gen_s.view(channel,height*width).mm(gen_s.view(channel,height*width).t())\n",
        "    #for style image\n",
        "    A=style_feature.view(channel,height*width).mm(style_feature.view(channel,height*width).t())\n",
        "\n",
        "    style_loss+=torch.mean((G-A)**2)\n",
        "\n",
        "  #backpropagation process\n",
        "  total_loss=alpha*content_loss + beta*style_loss\n",
        "  optimizer.zero_grad()\n",
        "  total_loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  #updating and saving the image after every 200 iterations to track progress\n",
        "  if step%200==0:\n",
        "    stp+=1\n",
        "    print('Step',stp,': Total loss:',total_loss,',Content loss:',content_loss,',Style loss:',style_loss)\n",
        "    save_image(generated_img,'/content/drive/MyDrive/Conv_images/NST/generated.jpg')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOczDDAtKWbKmITwJ3UBHA6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}